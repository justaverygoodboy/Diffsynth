import torch
from PIL import Image
from diffsynth import save_video, VideoData
from diffsynth.pipelines.wan_video_new import WanVideoPipeline, ModelConfig
from modelscope import dataset_snapshot_download
from diffsynth.models.model_manager import ModelManager
import imageio
pipe = WanVideoPipeline.from_pretrained(
    torch_dtype=torch.bfloat16,
    device="cuda",
    model_configs=[
        ModelConfig(path="./models/train/Wan2.1-Fun-1.3B-InP_full-concat/epoch-1.safetensors", offload_device="cpu"),
        ModelConfig(model_id="PAI/Wan2.1-Fun-1.3B-InP", origin_file_pattern="models_t5_umt5-xxl-enc-bf16.pth", offload_device="cpu"),
        ModelConfig(model_id="PAI/Wan2.1-Fun-1.3B-InP", origin_file_pattern="Wan2.1_VAE.pth", offload_device="cpu"),
        ModelConfig(model_id="PAI/Wan2.1-Fun-1.3B-InP", origin_file_pattern="models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth", offload_device="cpu"),
    ],
)
def extract_first_frame_from_video(video_path):
    """ä»è§†é¢‘æ–‡ä»¶ä¸­æå–ç¬¬ä¸€å¸§"""
    reader = imageio.get_reader(video_path)
    first_frame = reader.get_data(0)  # è·å–ç¬¬ä¸€å¸§
    reader.close()
        
    # è½¬æ¢ä¸ºPILå›¾åƒ
    frame_image = Image.fromarray(first_frame).convert("RGB")
    print(f"âœ… æˆåŠŸæå–ç¬¬ä¸€å¸§: {video_path}")
    return frame_image

def create_concatenated_input_image(rgb_img, depth_img):
    """åˆ›å»ºå‚ç›´æ‹¼æ¥çš„è¾“å…¥å›¾åƒï¼šRGBä¸Š + Depthä¸‹"""
    # ç¡®ä¿ä¸¤ä¸ªå›¾åƒå°ºå¯¸ä¸€è‡´
    width, height = rgb_img.size
    depth_img = depth_img.resize((width, height), Image.LANCZOS)
    
    # åˆ›å»ºæ‹¼æ¥å›¾åƒï¼š256x256 + 256x256 â†’ 256x512
    concatenated_height = height * 2
    concatenated_image = Image.new('RGB', (width, concatenated_height))
    
    # RGBåœ¨ä¸ŠåŠéƒ¨åˆ†
    concatenated_image.paste(rgb_img, (0, 0))
    
    # æ·±åº¦åœ¨ä¸‹åŠéƒ¨åˆ†
    concatenated_image.paste(depth_img, (0, height))
    
    print(f"ğŸ”§ åˆ›å»ºæ‹¼æ¥è¾“å…¥å›¾åƒ: {width}x{height} + {width}x{height} â†’ {width}x{concatenated_height}")
    return concatenated_image
def separate_concatenated_video(video_frames, rgb_height=256):
    """
    åˆ†ç¦»å‚ç›´æ‹¼æ¥çš„è§†é¢‘å¸§
    
    Args:
        video_frames: æ‹¼æ¥çš„è§†é¢‘å¸§åˆ—è¡¨
        rgb_height: RGBéƒ¨åˆ†çš„é«˜åº¦
    
    Returns:
        rgb_frames, depth_frames: åˆ†ç¦»åçš„RGBå’Œæ·±åº¦è§†é¢‘å¸§
    """
    rgb_frames = []
    depth_frames = []
    
    for frame in video_frames:
        # è·å–frameçš„numpyæ•°ç»„
        frame_array = np.array(frame)
        height, width = frame_array.shape[:2]
        
        # åˆ†ç¦»RGBï¼ˆä¸ŠåŠéƒ¨åˆ†ï¼‰å’Œæ·±åº¦ï¼ˆä¸‹åŠéƒ¨åˆ†ï¼‰
        rgb_part = frame_array[:rgb_height, :]
        depth_part = frame_array[rgb_height:, :]
        
        # è½¬æ¢å›PILå›¾åƒ
        rgb_frame = Image.fromarray(rgb_part)
        depth_frame = Image.fromarray(depth_part)
        
        rgb_frames.append(rgb_frame)
        depth_frames.append(depth_frame)
    
    return rgb_frames, depth_frames
# print("åŠ è½½è”åˆè®­ç»ƒæƒé‡...")
# model_manager = ModelManager()
# model_manager.load_model("./models/train/Wan2.1-Fun-1.3B-InP_full-joint/epoch-0.safetensors")
# pipe.dit = model_manager.fetch_model("wan_video_dit")
# print("åŠ è½½æ¨¡å‹å®Œæˆ...")
pipe.enable_vram_management()

rgb_video_path = "/gemini/platform/public/zqni/DiffSynth/data/HDTF_MEAD_train/M040_front_neutral_level_1_007.mp4"
depth_video_path = "/gemini/platform/public/zqni/DiffSynth/data/HDTF_MEAD_depth/M040_front_neutral_level_1_007_depth.mp4"
rgb_first_frame = extract_first_frame_from_video(rgb_video_path)
depth_first_frame = extract_first_frame_from_video(depth_video_path)
if rgb_first_frame and depth_first_frame:
    # è°ƒæ•´å›¾åƒå°ºå¯¸ä¸º256x256
    rgb_first_frame = rgb_first_frame.resize((256, 256), Image.LANCZOS)
    depth_first_frame = depth_first_frame.resize((256, 256), Image.LANCZOS)
        
    print(f"ğŸ–¼ï¸ RGBç¬¬ä¸€å¸§å°ºå¯¸: {rgb_first_frame.size}")
    print(f"ğŸ–¼ï¸ æ·±åº¦ç¬¬ä¸€å¸§å°ºå¯¸: {depth_first_frame.size}")
    # ä¿å­˜ç¬¬ä¸€å¸§å›¾åƒç”¨äºæŸ¥çœ‹
    rgb_first_frame.save("extracted_rgb_frame.png")
    depth_first_frame.save("extracted_depth_frame.png")
    print("ğŸ’¾ å·²ä¿å­˜æå–çš„ç¬¬ä¸€å¸§å›¾åƒ: extracted_rgb_frame.png, extracted_depth_frame.png")
    # åˆ›å»ºå‚ç›´æ‹¼æ¥çš„è¾“å…¥å›¾åƒ
    input_image = create_concatenated_input_image(rgb_first_frame, depth_first_frame)
    # ä¿å­˜æ‹¼æ¥å›¾åƒç”¨äºæŸ¥çœ‹
    input_image.save("concatenated_input_image.png")
    print("ğŸ’¾ å·²ä¿å­˜æ‹¼æ¥è¾“å…¥å›¾åƒ: concatenated_input_image.png")
    # ç”Ÿæˆè§†é¢‘
    print("ğŸ¬ å¼€å§‹ç”Ÿæˆè§†é¢‘...")

    # First and last frame to video
    video = pipe(
        prompt="A people is talking.",
        negative_prompt="è‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…ï¼Œå­—å¹•ï¼Œé£æ ¼ï¼Œä½œå“ï¼Œç”»ä½œï¼Œç”»é¢ï¼Œé™æ­¢ï¼Œæ•´ä½“å‘ç°ï¼Œæœ€å·®è´¨é‡ï¼Œä½è´¨é‡ï¼ŒJPEGå‹ç¼©æ®‹ç•™ï¼Œä¸‘é™‹çš„ï¼Œæ®‹ç¼ºçš„ï¼Œå¤šä½™çš„æ‰‹æŒ‡ï¼Œç”»å¾—ä¸å¥½çš„æ‰‹éƒ¨ï¼Œç”»å¾—ä¸å¥½çš„è„¸éƒ¨ï¼Œç•¸å½¢çš„ï¼Œæ¯å®¹çš„ï¼Œå½¢æ€ç•¸å½¢çš„è‚¢ä½“ï¼Œæ‰‹æŒ‡èåˆï¼Œé™æ­¢ä¸åŠ¨çš„ç”»é¢ï¼Œæ‚ä¹±çš„èƒŒæ™¯ï¼Œä¸‰æ¡è…¿ï¼ŒèƒŒæ™¯äººå¾ˆå¤šï¼Œå€’ç€èµ°",
        input_image=input_image,
        seed=0, tiled=True,
        height=512, width=256,
        # inference_mode="joint",  # "joint" for joint output, "rgb" for RGB only
        # You can input `end_image=xxx` to control the last frame of the video.
        # The model will automatically generate the dynamic content between `input_image` and `end_image`.
    )
# ä¿®å¤å­—å…¸é”®åè®¿é—®
# if isinstance(video, dict):
#     rgb_video = video["rgb_video"]  # ä¸æ˜¯ "rgb_video"
#     depth_video = video["depth_video"]  # ä¸æ˜¯ "depth_video"
#     save_video(rgb_video, "video_rgb.mp4", fps=15, quality=5)
#     save_video(depth_video, "video_depth.mp4", fps=15, quality=5)
# else:
    # å¦‚æœä¸æ˜¯è”åˆè¾“å‡ºï¼Œç›´æ¥ä¿å­˜
    save_video(video, "video_concat.mp4", fps=15, quality=5)
